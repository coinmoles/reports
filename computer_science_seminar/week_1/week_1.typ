#import "../../templates/template-report.typ": *

#show: template-report.with(
  "Week 1 Report",
  student_id: "2023-12675"
  author: "박지호"
)

Today's seminar focused on the current state of large language model (LLM) technology, and how we can adapt to the AI ecosystem. 

The very first topic of the lecture was introduction of Strawberry, the brand new LLM model that was just released today. The model specializes in tasks that require reasoning, and showed remarkable results in competitive math and programming. Even though it feels like you can encounter these otherworldly performance metrics almost every other day, these results were truly impressive to me. I doubt I can do half as good as what it is doing. 

Then, the presenter discussed about several training methods used to improve performance of these models. For the strawberry model, the model was made to ask questions to itself and answer it repeatedly, which allowed it to come up with step-by-step solutions to a problem. We were also introduced to other learning strategies like RAG, where an additional authorized source is used outside of regular training data, and fine tuning methods like continued fine tuning, where the model is continuously fine tuned using user input.

Finally, the presenter wrapped off the lecture by showing us ways we can adapt to the AI ecosystem. They first showed us few of the toy projects utilizing LLM that they've been working on, and how easy it is to make one of them. We were encouraged to try making an application like this ourselves, or better, work on optimizing the AI model itself. It was also mentioned that if we are not going to do that, we should at least familiarize ourselves with AI powered tools.

Nowadays, you can find a lot of anti-AI propaganda around the media. Although I always tried to not take them too seriously, I think I was subconsciously affected by them and became reluctant to embrace AI-powered tools. As a CS major, I should have been the last person to have this attitude. Also, I simply didn't think I would find it enjoyable. One of my favorite aspects about programming is planning out an architecture of a program in a way that the resulting code is simple readible, which, at least to my understanding, isn't really present in AI programming. It didn't help that I heard a lot of bad rumors about the AI industry, like the infamous 'attention is all you need' claim.

However, I now realize that working with AI is no longer a choice, but a must. I didn't realize how much easier AI would make my life if only I utilized it. Also, this seminar has enlightened me of a way to do so in a much lighthearted way than actually desigining and training the model itself. Therefore, I am also motivated to look into ways to integrate AI into the projects I may start in the future.

Although I have the very some, albeit elementary, understandings of the AI technology, I am completely in the dark when it comems to LLM. I wonder what would be a good starting point to get into it. What resources should I look at to learn how to handle LLM? What tools would be simple enough to make as a learning project? I should start researching after this, and I am sure this lecture will be a good source of information too.

Also, I wonder why the generic public is so negative about anything AI related. I understand why artists may be upset of AI art, but I feel like the bad reception has spread into the AI itself and the areas that should not be problematic at all. The media tends to highlight the worst aspects of AI, rather than the effectiveness of the technology. I am curious what are the AI developers' opinion on this matter, and whether there are ways to improve the reception.